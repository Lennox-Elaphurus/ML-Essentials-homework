{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6beae10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd17f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f24114c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ff930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import conv2d, max_pool2d, cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34763ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"figure\", dpi=100)      \n",
    "batch_size = 100            #It is used to define the number of samples/images that will be processed in each batch during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf739fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform images into normalized tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),     #It converts the image from PIL (Python Imaging Library) format or numpy array format to a tensor representation that can be used as input to a neural network.\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))    #This transformation normalizes the tensor values by subtracting the mean and dividing by the standard deviation. In this case, the mean is set to 0.5, and the standard deviation is also set to 0.5.\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a085e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ffebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4038cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90ab9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed8d7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # Kaiming He initialization (a good initialization is important)\n",
    "    # https://arxiv.org/abs/1502.01852\n",
    "    std = np.sqrt(2. / shape[0])\n",
    "    w = torch.randn(size=shape) * std\n",
    "    w.requires_grad = True\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d883222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectify(x):\n",
    "    # Rectified Linear Unit (ReLU)\n",
    "    return torch.max(torch.zeros_like(x), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "273b54bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSprop(optim.Optimizer):\n",
    "    \"\"\"\n",
    "    This is a reduced version of the PyTorch internal RMSprop optimizer\n",
    "    It serves here as an example\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # state initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(grad, avg, value=-group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e14e2500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural network\n",
    "def model(x, w_h, w_h2, w_o):\n",
    "    h = rectify(x @ w_h)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "849195fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights\n",
    "\n",
    "# input shape is (B, 784)\n",
    "w_h = init_weights((784, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_h2 = init_weights((625, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_o = init_weights((625, 10))\n",
    "# output shape is (B, 10)\n",
    "optimizer = RMSprop(params=[w_h, w_h2, w_o])\n",
    "n_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92255cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Mean Train Loss: 3.96e-01\n",
      "Mean Test Loss:  2.39e-01\n",
      "Epoch: 10\n",
      "Mean Train Loss: 1.45e-01\n",
      "Mean Test Loss:  3.67e-01\n",
      "Epoch: 20\n",
      "Mean Train Loss: 1.00e-01\n",
      "Mean Test Loss:  4.36e-01\n",
      "Epoch: 30\n",
      "Mean Train Loss: 6.88e-02\n",
      "Mean Test Loss:  5.47e-01\n",
      "Epoch: 40\n",
      "Mean Train Loss: 4.71e-02\n",
      "Mean Test Loss:  7.53e-01\n",
      "Epoch: 50\n",
      "Mean Train Loss: 4.30e-02\n",
      "Mean Test Loss:  6.80e-01\n",
      "Epoch: 60\n",
      "Mean Train Loss: 3.35e-02\n",
      "Mean Test Loss:  7.38e-01\n",
      "Epoch: 70\n",
      "Mean Train Loss: 2.02e-02\n",
      "Mean Test Loss:  8.58e-01\n",
      "Epoch: 80\n",
      "Mean Train Loss: 1.84e-02\n",
      "Mean Test Loss:  8.45e-01\n",
      "Epoch: 90\n",
      "Mean Train Loss: 1.31e-02\n",
      "Mean Test Loss:  8.68e-01\n",
      "Epoch: 100\n",
      "Mean Train Loss: 8.88e-03\n",
      "Mean Test Loss:  1.01e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2381497a5b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put this into a training loop over 100 epochs\n",
    "for epoch in range(n_epochs + 1):\n",
    "    train_loss_this_epoch = []\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        x, y = batch\n",
    "\n",
    "        # our model requires flattened input\n",
    "        x = x.reshape(batch_size, 784)\n",
    "        # feed input through model\n",
    "        noise_py_x = model(x, w_h, w_h2, w_o)\n",
    "\n",
    "        # reset the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "\n",
    "        train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(np.mean(train_loss_this_epoch))\n",
    "     # test periodically\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "        test_loss_this_epoch = []\n",
    "\n",
    "        # no need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(test_dataloader):\n",
    "                x, y = batch\n",
    "                x = x.reshape(batch_size, 784)\n",
    "                noise_py_x = model(x, w_h, w_h2, w_o)\n",
    "\n",
    "                loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "        print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "\n",
    "plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "plt.title(\"Train and Test Loss over Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
