{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "997c6b5a",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e11123",
   "metadata": {},
   "source": [
    "Authors: \\\n",
    "Tuoxing Liu \\\n",
    "Sima Esmaeili \\\n",
    "Shruti Ghargi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc97042",
   "metadata": {},
   "source": [
    "# 1 Classification and Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b0df1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from abc import abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ffcf16",
   "metadata": {},
   "source": [
    "## Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea27189",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "      this class will later get the following attributes\n",
    "      all nodes:\n",
    "          features\n",
    "          responses\n",
    "      split nodes additionally:\n",
    "          left\n",
    "          right\n",
    "          split_index\n",
    "          threshold\n",
    "      leaf nodes additionally\n",
    "          prediction\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "\n",
    "class Tree:\n",
    "    '''\n",
    "      base class for RegressionTree and ClassificationTree\n",
    "    '''\n",
    "    def __init__(self, n_min=10):\n",
    "        '''n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        self.n_min = n_min \n",
    "    \n",
    "    def predict(self, x):\n",
    "        ''' return the prediction for the given 1-D feature vector x\n",
    "        '''\n",
    "        # first find the leaf containing the 1-D feature vector x\n",
    "        node = self.root\n",
    "        while not hasattr(node, \"prediction\"):\n",
    "            j = node.split_index\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        # finally, return the leaf's prediction\n",
    "        return node.prediction\n",
    "        \n",
    "    def train(self, features, responses, D_try=None):\n",
    "        '''\n",
    "        features: the feature matrix of the training set\n",
    "        response: the vector of responses\n",
    "        '''\n",
    "        N, D = features.shape\n",
    "        assert(responses.shape[0] == N)\n",
    "\n",
    "        if D_try is None:\n",
    "            D_try = int(np.sqrt(D))  # number of features to consider for each split decision\n",
    "        \n",
    "        # initialize the root node\n",
    "        self.root = Node()\n",
    "        self.root.features = features\n",
    "        self.root.responses = responses\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            active_indices = self.select_active_indices(D, D_try)\n",
    "            left, right = self.make_split_node(node, active_indices)\n",
    "            if left is None:  # no split found\n",
    "                self.make_leaf_node(node)\n",
    "            else:\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "    \n",
    "    def make_split_node(self, node, indices):\n",
    "        '''\n",
    "        node: the node to be split\n",
    "        indices: a numpy array of length 'D_try', containing the feature \n",
    "                         indices to be considered for the present split\n",
    "                         \n",
    "        return: None, None -- if no suitable split has been found, or\n",
    "                left, right -- the children of the split\n",
    "        '''\n",
    "        # all responses equal => no improvement possible by any split\n",
    "        if np.unique(node.responses).shape[0] == 1:\n",
    "            return None, None\n",
    "        \n",
    "        # find best feature j_min (among 'indices') and best threshold t_min for the split\n",
    "        l_min = float('inf')  # upper bound for the loss, later the loss of the best split\n",
    "        j_min, t_min = None, None\n",
    "\n",
    "        for j in indices:\n",
    "            thresholds = self.find_thresholds(node, j)\n",
    "\n",
    "            # compute loss for each threshold\n",
    "            for t in thresholds:\n",
    "                loss = self.compute_loss_for_split(node, j, t)\n",
    "\n",
    "                # remember the best split so far \n",
    "                # (the condition is never True when loss = float('inf') )\n",
    "                if loss < l_min:\n",
    "                    l_min = loss\n",
    "                    j_min = j\n",
    "                    t_min = t\n",
    "\n",
    "        if j_min is None:  # no split found\n",
    "            return None, None\n",
    "\n",
    "        # create children for the best split\n",
    "        left, right = self.make_children(node, j_min, t_min)\n",
    "\n",
    "        # turn the current 'node' into a split node\n",
    "        node.split_index = j_min\n",
    "        node.threshold = t_min\n",
    "        node.left = left\n",
    "        node.right = right\n",
    "        \n",
    "        # return the children (to be placed on the stack)\n",
    "        return left, right\n",
    "    \n",
    "    def select_active_indices(self, D, D_try):\n",
    "        ''' return a 1-D array with D_try randomly selected indices from 0...(D-1).\n",
    "        '''\n",
    "        return np.random.choice(D, D_try, replace=False)\n",
    "    \n",
    "    def find_thresholds(self, node, j):\n",
    "        ''' return: a 1-D array with all possible thresholds along feature j\n",
    "        '''\n",
    "        return np.sort(np.unique(node.features[:, j]))\n",
    "    \n",
    "    def make_children(self, node, j, t):\n",
    "        ''' execute the split in feature j at threshold t\n",
    "        \n",
    "            return: left, right -- the children of the split, with features and responses\n",
    "                                   properly assigned according to the split\n",
    "        '''\n",
    "        left = Node()\n",
    "        right = Node()\n",
    "\n",
    "        indices_left = node.features[:, j] <= t\n",
    "        indices_right = ~indices_left\n",
    "        \n",
    "        left.features = node.features[indices_left]\n",
    "        left.responses = node.responses[indices_left]\n",
    "        \n",
    "        right.features = node.features[indices_right]\n",
    "        right.responses = node.responses[indices_right]\n",
    "        \n",
    "        return left, right\n",
    "        \n",
    "    @abstractmethod\n",
    "    def make_leaf_node(self, node):\n",
    "        ''' Turn node into a leaf by computing and setting `node.prediction`\n",
    "        \n",
    "            (must be implemented in a subclass)\n",
    "        '''\n",
    "        raise NotImplementedError(\"make_leaf_node() must be implemented in a subclass.\")\n",
    "        \n",
    "    @abstractmethod\n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        ''' Return the resulting loss when the data are split along feature j at threshold t.\n",
    "            If the split is not admissible, return float('inf').\n",
    "        \n",
    "            (must be implemented in a subclass)\n",
    "        '''\n",
    "        raise NotImplementedError(\"compute_loss_for_split() must be implemented in a subclass.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70593f04",
   "metadata": {},
   "source": [
    "## Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98bd406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree(Tree):\n",
    "    def __init__(self, n_min=10):\n",
    "        super(RegressionTree, self).__init__(n_min)\n",
    "        \n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        # return the loss if we would split the instance along feature j at threshold t\n",
    "        # or float('inf') if there is no feasible split\n",
    "        left_indices = node.features[:, j] <= t\n",
    "        right_indices = ~left_indices\n",
    "\n",
    "        if np.sum(left_indices) < self.n_min or np.sum(right_indices) < self.n_min:\n",
    "            return float('inf')\n",
    "\n",
    "        left_responses = node.responses[left_indices]\n",
    "        right_responses = node.responses[right_indices]\n",
    "\n",
    "        left_loss = np.mean((left_responses - np.mean(left_responses))**2)\n",
    "        right_loss = np.mean((right_responses - np.mean(right_responses))**2)\n",
    "\n",
    "        return left_loss + right_loss\n",
    "    \n",
    "    def make_leaf_node(self, node):\n",
    "        # turn node into a leaf node by computing `node.prediction`\n",
    "        # (note: the prediction of a regression tree is a real number)\n",
    "        node.prediction = np.mean(node.responses)\n",
    "        \n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        # return the loss if we would split the instance along feature j at threshold t\n",
    "        # or float('inf') if there is no feasible split\n",
    "        left_indices = node.features[:, j] <= t\n",
    "        right_indices = ~left_indices\n",
    "\n",
    "        if np.sum(left_indices) < self.n_min or np.sum(right_indices) < self.n_min:\n",
    "            return float('inf')\n",
    "\n",
    "        left_responses = node.responses[left_indices]\n",
    "        right_responses = node.responses[right_indices]\n",
    "\n",
    "        left_loss = np.mean((left_responses - np.mean(left_responses))**2)\n",
    "        right_loss = np.mean((right_responses - np.mean(right_responses))**2)\n",
    "\n",
    "        return left_loss + right_loss\n",
    "    \n",
    "    def make_leaf_node(self, node):\n",
    "        # turn node into a leaf node by computing `node.prediction`\n",
    "        # (note: the prediction of a regression tree is a real number)\n",
    "        node.prediction = np.mean(node.responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83ec3a",
   "metadata": {},
   "source": [
    "## Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e95928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTree(Tree):\n",
    "    '''implement classification tree so that it can handle arbitrary many classes\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, classes, n_min=10):\n",
    "        ''' classes: a 1-D array with the permitted class labels\n",
    "            n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        super(ClassificationTree, self).__init__(n_min)\n",
    "        self.classes = classes\n",
    "        \n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        # return the loss if we would split the instance along feature j at threshold t\n",
    "        # or float('inf') if there is no feasible split\n",
    "        left_indices = node.features[:, j] <= t\n",
    "        right_indices = ~left_indices\n",
    "\n",
    "        if np.sum(left_indices) < self.n_min or np.sum(right_indices) < self.n_min:\n",
    "            return float('inf')\n",
    "\n",
    "        left_responses = node.responses[left_indices]\n",
    "        right_responses = node.responses[right_indices]\n",
    "\n",
    "        left_loss = self.compute_loss(left_responses)\n",
    "        right_loss = self.compute_loss(right_responses)\n",
    "\n",
    "        return (np.sum(left_indices) * left_loss + np.sum(right_indices) * right_loss) / np.sum(left_indices + right_indices)\n",
    "    \n",
    "    def compute_loss(self, responses):\n",
    "        class_counts = np.bincount(responses, minlength=len(self.classes))\n",
    "        class_probabilities = class_counts / np.sum(class_counts)\n",
    "\n",
    "        return self.compute_entropy(class_probabilities)\n",
    "    \n",
    "    def compute_entropy(self, probabilities):\n",
    "        probabilities = probabilities[probabilities > 0]\n",
    "        return -np.sum(probabilities * np.log2(probabilities))\n",
    "    \n",
    "    def make_leaf_node(self, node):\n",
    "        # turn node into a leaf node by computing `node.prediction`\n",
    "        # (note: the prediction of a classification tree is a class label)\n",
    "        class_counts = np.bincount(node.responses, minlength=len(self.classes))\n",
    "        node.prediction = np.argmax(class_counts)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed5d858",
   "metadata": {},
   "source": [
    "## Evaluation of Regression and Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b7e162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "# read and prepare the digits data and extract 3s and 9s\n",
    "digits = load_digits()\n",
    "print(digits.data.shape, digits.target.shape)\n",
    "\n",
    "instances = (digits.target == 3) | (digits.target == 9)\n",
    "features = digits.data[instances, :]\n",
    "labels = digits.target[instances]\n",
    "\n",
    "# for regression, we use labels +1 and -1\n",
    "responses = np.array([1 if l == 3 else -1 for l in labels])\n",
    "\n",
    "assert(features.shape[0] == labels.shape[0] == responses.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945762d",
   "metadata": {},
   "source": [
    "For Regression Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "283cf6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Tree Mean Accuracy: 0.38923872309061275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create an instance of RegressionTree\n",
    "regression_tree = RegressionTree()\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "kf = KFold(n_splits=5)\n",
    "mse_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(features):\n",
    "    train_features, test_features = features[train_index], features[test_index]\n",
    "    train_responses, test_responses = responses[train_index], responses[test_index]\n",
    "\n",
    "    # Train the regression tree\n",
    "    regression_tree.train(train_features, train_responses)\n",
    "\n",
    "    # Predict the responses for the test set\n",
    "    predictions = [regression_tree.predict(x) for x in test_features]\n",
    "\n",
    "    # Calculate the mean squared error (MSE)\n",
    "    mse = np.mean((test_responses - predictions) ** 2)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "# Calculate and print the mean MSE score\n",
    "mean_mse = np.mean(mse_scores)\n",
    "print(\"Regression Tree Mean Accuracy:\", mean_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6213ed",
   "metadata": {},
   "source": [
    "For Classification Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4931d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Tree Mean Accuracy: 0.8762176560121766\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create an instance of ClassificationTree\n",
    "classification_tree = ClassificationTree(classes=np.unique(labels))\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "accuracy_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(features, labels):\n",
    "    train_features, test_features = features[train_index], features[test_index]\n",
    "    train_labels, test_labels = labels[train_index], labels[test_index]\n",
    "\n",
    "    # Train the classification tree\n",
    "    classification_tree.train(train_features, train_labels)\n",
    "\n",
    "    # Predict the labels for the test set\n",
    "    predictions = [classification_tree.predict(x) for x in test_features]\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = np.mean(predictions == test_labels)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "# Calculate and print the mean accuracy score\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "print(\"Classification Tree Mean Accuracy:\", mean_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
