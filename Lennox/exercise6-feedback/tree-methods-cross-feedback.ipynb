{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from abc import abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "      this class will later get the following attributes\n",
    "      all nodes:\n",
    "          features\n",
    "          responses\n",
    "      split nodes additionally:\n",
    "          left\n",
    "          right\n",
    "          split_index\n",
    "          threshold\n",
    "      leaf nodes additionally\n",
    "          prediction\n",
    "    '''\n",
    "\n",
    "\n",
    "class Tree:\n",
    "    '''\n",
    "      base class for RegressionTree and ClassificationTree\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_min=10):\n",
    "        '''n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        self.n_min = n_min\n",
    "\n",
    "    def predict(self, x):\n",
    "        ''' return the prediction for the given 1-D feature vector x\n",
    "        '''\n",
    "        # first find the leaf containing the 1-D feature vector x\n",
    "        node = self.root\n",
    "        while not hasattr(node, \"prediction\"):\n",
    "            j = node.split_index\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        # finally, return the leaf's prediction\n",
    "        return node.prediction\n",
    "\n",
    "    def train(self, features, responses, D_try=None):\n",
    "        '''\n",
    "        features: the feature matrix of the training set\n",
    "        response: the vector of responses\n",
    "        '''\n",
    "        N, D = features.shape\n",
    "        assert (responses.shape[0] == N)\n",
    "\n",
    "        if D_try is None:\n",
    "            D_try = int(np.sqrt(D))  # number of features to consider for each split decision\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root = Node()\n",
    "        self.root.features = features\n",
    "        self.root.responses = responses\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            active_indices = self.select_active_indices(D, D_try)\n",
    "            left, right = self.make_split_node(node, active_indices)\n",
    "            if left is None:  # no split found\n",
    "                self.make_leaf_node(node)\n",
    "            else:\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "\n",
    "    def make_split_node(self, node, indices):\n",
    "        '''\n",
    "        node: the node to be split\n",
    "        indices: a numpy array of length 'D_try', containing the feature\n",
    "                         indices to be considered for the present split\n",
    "\n",
    "        return: None, None -- if no suitable split has been found, or\n",
    "                left, right -- the children of the split\n",
    "        '''\n",
    "        # all responses equal => no improvement possible by any split\n",
    "        if np.unique(node.responses).shape[0] == 1:\n",
    "            return None, None\n",
    "\n",
    "        # find best feature j_min (among 'indices') and best threshold t_min for the split\n",
    "        l_min = float('inf')  # upper bound for the loss, later the loss of the best split\n",
    "        j_min, t_min = None, None\n",
    "\n",
    "        for j in indices:\n",
    "            thresholds = self.find_thresholds(node, j)\n",
    "\n",
    "            # compute loss for each threshold\n",
    "            for t in thresholds:\n",
    "                loss = self.compute_loss_for_split(node, j, t)\n",
    "\n",
    "                # remember the best split so far\n",
    "                # (the condition is never True when loss = float('inf') )\n",
    "                if loss < l_min:\n",
    "                    l_min = loss\n",
    "                    j_min = j\n",
    "                    t_min = t\n",
    "\n",
    "        if j_min is None:  # no split found\n",
    "            return None, None\n",
    "\n",
    "        # create children for the best split\n",
    "        left, right = self.make_children(node, j_min, t_min)\n",
    "\n",
    "        # turn the current 'node' into a split node\n",
    "        # (store children and split condition)\n",
    "        ...  # your code here\n",
    "\n",
    "        node.left=left\n",
    "        node.right=right\n",
    "        node.split_index=j_min\n",
    "        node.threshold=t_min\n",
    "\n",
    "        #raise NotImplementedError(\"make_split_node(): remove this exception after adding your code above.\")\n",
    "\n",
    "        # return the children (to be placed on the stack)\n",
    "        return left, right\n",
    "\n",
    "    def select_active_indices(self, D, D_try):\n",
    "        ''' return a 1-D array with D_try randomly selected indices from 0...(D-1).\n",
    "        '''\n",
    "        ...  # your code here\n",
    "        active_indices = np.random.choice(D, D_try, replace=False)\n",
    "        return active_indices\n",
    "        #raise NotImplementedError(\"select_active_indices(): remove this exception after adding your code above.\")\n",
    "\n",
    "    def find_thresholds(self, node, j):\n",
    "        ''' return: a 1-D array with all possible thresholds along feature j\n",
    "        '''\n",
    "        ...  # your code here\n",
    "        sorted_feature = np.sort(node.features[:,j])\n",
    "        thresholds = (sorted_feature[1:] + sorted_feature[:-1]) / 2.0\n",
    "\n",
    "        return thresholds\n",
    "        #raise NotImplementedError(\"find_thresholds(): remove this exception after adding your code above.\")\n",
    "\n",
    "    def make_children(self, node, j, t):\n",
    "        ''' execute the split in feature j at threshold t\n",
    "\n",
    "            return: left, right -- the children of the split, with features and responses\n",
    "                                   properly assigned according to the split\n",
    "        '''\n",
    "        left = Node()\n",
    "        right = Node()\n",
    "        left.features = node.features[node.features[:, j] <= t]\n",
    "        left.responses = node.responses[node.features[:, j] <= t]\n",
    "\n",
    "        right.features = node.features[node.features[:, j] > t]\n",
    "        right.responses = node.responses[node.features[:, j] > t]\n",
    "        # your code here\n",
    "        #raise NotImplementedError(\"make_children(): remove this exception after adding your code above.\")\n",
    "\n",
    "        return left, right\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_leaf_node(self, node):\n",
    "        ''' Turn node into a leaf by computing and setting `node.prediction`\n",
    "\n",
    "            (must be implemented in a subclass)\n",
    "        '''\n",
    "        raise NotImplementedError(\"make_leaf_node() must be implemented in a subclass.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        ''' Return the resulting loss when the data are split along feature j at threshold t.\n",
    "            If the split is not admissible, return float('inf').\n",
    "\n",
    "            (must be implemented in a subclass)\n",
    "        '''\n",
    "        raise NotImplementedError(\"compute_loss_for_split() must be implemented in a subclass.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree(Tree):\n",
    "    def __init__(self, n_min=10):\n",
    "        super(RegressionTree, self).__init__(n_min)\n",
    "\n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        # return the loss if we would split the instance along feature j at threshold t\n",
    "        # or float('inf') if there is no feasible split\n",
    "        ...  # your code here\n",
    "        feature_j=node.features[:,j]\n",
    "        left=feature_j[feature_j<t]\n",
    "        right=feature_j[feature_j>=t]\n",
    "        if (len(left) < self.n_min) | (len(right) < self.n_min):\n",
    "            return float('inf')\n",
    "        loss = np.var(left) * len(left) + np.var(right) * len(right)\n",
    "        return loss\n",
    "        #raise NotImplementedError(\"compute_loss_for_split(): remove this exception after adding your code above.\")\n",
    "\n",
    "    def make_leaf_node(self, node):\n",
    "        # turn node into a leaf node by computing `node.prediction`\n",
    "        # (note: the prediction of a regression tree is a real number)\n",
    "        # your code here\n",
    "        node.prediction = np.mean(node.responses)\n",
    "        #raise NotImplementedError(\"make_leaf_node(): remove this exception after adding your code above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationTree(Tree):\n",
    "    '''implement classification tree so that it can handle arbitrary many classes\n",
    "    '''\n",
    "\n",
    "    def __init__(self, classes, n_min=10):\n",
    "        ''' classes: a 1-D array with the permitted class labels\n",
    "            n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        super(ClassificationTree, self).__init__(n_min)\n",
    "        self.classes = classes\n",
    "\n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        # return the loss if we would split the instance along feature j at threshold t\n",
    "        # or float('inf') if there is no feasible split\n",
    "        left_responses = node.responses[node.features[:, j] <= t]\n",
    "        right_responses = node.responses[node.features[:, j] > t]\n",
    "        #feature_j = node.features[:, j]\n",
    "        #left = feature_j[feature_j < t]\n",
    "        #right = feature_j[feature_j >= t]\n",
    "        #left_response = node.responses[left]\n",
    "        #right_response = node.responses[right]\n",
    "        if (len(left_responses) < self.n_min) or (len(right_responses) < self.n_min):\n",
    "            return float('inf')\n",
    "\n",
    "        left_count = np.bincount(left_responses)\n",
    "        left_prob = left_count / len(left_responses)\n",
    "        left_impurity = 1 - np.sum(left_prob ** 2)\n",
    "\n",
    "        right_count = np.bincount(right_responses)\n",
    "        right_prob = right_count / len(right_responses)\n",
    "        right_impurity = 1 - np.sum(right_prob ** 2)\n",
    "\n",
    "        total = len(left_responses) + len(right_responses)\n",
    "        loss = (len(left_responses) / total) * left_impurity + (len(right_responses) / total) * right_impurity\n",
    "\n",
    "        return loss\n",
    "    def make_leaf_node(self, node):\n",
    "        # turn node into a leaf node by computing `node.prediction`\n",
    "        # (note: the prediction of a classification tree is a class label)\n",
    "        node.prediction=np.argmax(np.bincount(node.responses))\n",
    "        # your code here\n",
    "        #raise NotImplementedError(\"make_leaf_node(): remove this exception after adding your code above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Regression and Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "# read and prepare the digits data and extract 3s and 9s\n",
    "digits = load_digits()\n",
    "print(digits.data.shape, digits.target.shape)\n",
    "\n",
    "instances = (digits.target == 3) | (digits.target == 9)\n",
    "features = digits.data[instances, :]\n",
    "labels = digits.target[instances]\n",
    "\n",
    "# for regression, we use labels +1 and -1\n",
    "responses = np.array([1 if l == 3 else -1 for l in labels])\n",
    "\n",
    "assert(features.shape[0] == labels.shape[0] == responses.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Tree: 0.6749277873883317\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation (see ex01) with responses +1 and -1 (for 3s and 9s)\n",
    "# using RegressionTree()\n",
    "# and comment on your results\n",
    "# your code here\n",
    "def regression_tree(features, responses, n_folds=5, n_min=10):\n",
    "    fold_size = features.shape[0] // n_folds\n",
    "    indices = np.arange(features.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    total_acc = 0.0\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        fold_start = i * fold_size\n",
    "        fold_end = (i + 1) * fold_size\n",
    "\n",
    "        validation_indices = indices[fold_start:fold_end]\n",
    "        train_indices = np.concatenate((indices[:fold_start], indices[fold_end:]))\n",
    "\n",
    "        train_features = features[train_indices]\n",
    "        train_responses = responses[train_indices]\n",
    "        test_features = features[validation_indices]\n",
    "        test_responses = responses[validation_indices]\n",
    "\n",
    "        tree = RegressionTree(n_min)\n",
    "        tree.train(train_features, train_responses)\n",
    "\n",
    "        predictions = np.array([tree.predict(x) for x in test_features])\n",
    "        accuracy = np.mean((predictions - test_responses) ** 2)\n",
    "\n",
    "        total_acc += accuracy\n",
    "\n",
    "    average_accuracy = total_acc / n_folds\n",
    "    return average_accuracy\n",
    "accuracy_regressiontree = regression_tree(features, responses)\n",
    "print(\"Regression Tree:\", accuracy_regressiontree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Tree: 0.8527777777777779\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation with labels 3 and 9\n",
    "# using ClassificationTree(classes=np.unique(labels))\n",
    "# and comment on your results\n",
    "# your code here\n",
    "def classification_tree(features, labels, classes, n_folds=5, n_min=10):\n",
    "    fold_size = features.shape[0] // n_folds\n",
    "    indices = np.arange(features.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    total_accuracy = 0.0\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        fold_start = i * fold_size\n",
    "        fold_end = (i + 1) * fold_size\n",
    "\n",
    "        validation_indices = indices[fold_start:fold_end]\n",
    "        train_indices = np.concatenate((indices[:fold_start], indices[fold_end:]))\n",
    "\n",
    "        train_features = features[train_indices]\n",
    "        train_labels = labels[train_indices]\n",
    "        test_features = features[validation_indices]\n",
    "        test_labels = labels[validation_indices]\n",
    "\n",
    "        tree = ClassificationTree(classes, n_min)\n",
    "        tree.train(train_features, train_labels)\n",
    "        predictions = np.array([tree.predict(x) for x in test_features])\n",
    "        accuracy = np.mean(predictions == test_labels)\n",
    "\n",
    "        total_accuracy += accuracy\n",
    "\n",
    "    average_accuracy = total_accuracy / n_folds\n",
    "    return average_accuracy\n",
    "\n",
    "\n",
    "accuracy_classficationtree = classification_tree(features,labels, np.unique(labels))\n",
    "print(\"Classification Tree:\", accuracy_classficationtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression and Classification Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sampling(features, responses):\n",
    "    '''return a bootstrap sample of features and responses\n",
    "    '''\n",
    "    # your code here\n",
    "    N=len(features)\n",
    "    indices=np.random.choice(N, size=N, replace=True)\n",
    "    bootstrap_features = features[indices]\n",
    "    bootstrap_responses = responses[indices]\n",
    "    return bootstrap_features, bootstrap_responses\n",
    "\n",
    "    #raise NotImplementedError(\"bootstrap_sampling(): remove this exception after adding your code above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RegressionForest():\n",
    "    def __init__(self, n_trees, n_min=10):\n",
    "        # create ensemble\n",
    "        self.trees = [RegressionTree(n_min) for i in range(n_trees)]\n",
    "\n",
    "    def train(self, features, responses):\n",
    "        for tree in self.trees:\n",
    "            boostrap_features, bootstrap_responses = bootstrap_sampling(features, responses)\n",
    "            tree.train(boostrap_features, bootstrap_responses)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the response of the ensemble from the individual responses and return it\n",
    "        # your code here\n",
    "        predictions = [tree.predict(x) for tree in self.trees]\n",
    "        return np.mean(predictions)\n",
    "        #raise NotImplementedError(\"predict(): remove this exception after adding your code above.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationForest():\n",
    "    def __init__(self, n_trees, classes, n_min=1):\n",
    "        self.trees = [ClassificationTree(classes, n_min) for i in range(n_trees)]\n",
    "        self.classes = classes\n",
    "\n",
    "    def train(self, features, responses):\n",
    "        for tree in self.trees:\n",
    "            boostrap_features, bootstrap_responses = bootstrap_sampling(features, responses)\n",
    "            tree.train(boostrap_features, bootstrap_responses)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the response of the ensemble from the individual responses and return it\n",
    "        # your code here\n",
    "        predictions = [tree.predict(x) for tree in self.trees]\n",
    "        return np.argmax(np.bincount(predictions))\n",
    "        #raise NotImplementedError(\"predict(): remove this exception after adding your code above.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Regression and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regresion Forest: 0.862290715372907\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation (see ex01) with responses +1 and -1 (for 3s and 9s)\n",
    "# using RegressionForest(n_trees=10)\n",
    "# and comment on your results\n",
    "# your code here\n",
    "def regression_forest(features,responses,n_folds=5,n_trees=10):\n",
    "    accuracy_scores = [ ]\n",
    "\n",
    "    kfold = KFold(n_folds)\n",
    "    for train_index, test_index in kfold.split(features):\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = responses[train_index], responses[test_index]\n",
    "\n",
    "        forest = RegressionForest(n_trees=n_trees)\n",
    "        forest.train(X_train, y_train)\n",
    "\n",
    "        y_pred = [forest.predict(x) for x in X_test]\n",
    "\n",
    "        accuracy = np.mean(np.sign(y_pred) == y_test)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        #accuracy_scores+=accuracy\n",
    "\n",
    "    average_accuracy = np.mean(accuracy_scores)\n",
    "    return average_accuracy\n",
    "\n",
    "accuracy_regressionforest=regression_forest(features,responses)\n",
    "print('Regresion Forest:',accuracy_regressionforest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Forest: 0.9505707762557078\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation with labels 3 and 9\n",
    "# using DecisionForest(n_trees=10, classes=np.unique(labels))\n",
    "# and comment on your results\n",
    "# your code here\n",
    "def classification_forest(features,labels,n_folds=5,n_trees=10):\n",
    "    accuracy_scores = [ ]\n",
    "\n",
    "    kfold = KFold(n_folds)\n",
    "    for train_index, test_index in kfold.split(features):\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "        forest = ClassificationForest(n_trees,np.unique(digits.target))\n",
    "        forest.train(X_train, y_train)\n",
    "\n",
    "        y_pred = [forest.predict(x) for x in X_test]\n",
    "\n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        #accuracy_scores+=accuracy\n",
    "\n",
    "    average_accuracy = np.mean(accuracy_scores)\n",
    "    return average_accuracy\n",
    "\n",
    "accuracy_classficationforest=classification_forest(features,labels)\n",
    "print('Classification Forest:',accuracy_classficationforest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Classification Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Forest(arbitrary): 0.9015382234602292\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[35.   0.   0.   0.   0.4  0.   0.   0.   0.   0.2]\n",
      " [ 0.  33.4  0.8  0.   0.   0.2  0.8  0.   0.6  0.6]\n",
      " [ 0.2  0.6 32.4  1.   0.   0.   0.   0.   1.2  0. ]\n",
      " [ 0.2  0.2  0.6 30.8  0.   0.8  0.2  0.6  2.   1.2]\n",
      " [ 0.   0.2  0.2  0.  34.6  0.2  0.   0.8  0.   0.2]\n",
      " [ 0.6  0.   0.   0.4  0.8 32.4  0.2  0.   0.6  1.4]\n",
      " [ 0.2  0.8  0.   0.   0.6  0.6 33.8  0.   0.2  0. ]\n",
      " [ 0.   0.8  0.2  0.2  1.2  0.   0.  32.2  0.   1.2]\n",
      " [ 0.   2.6  1.4  0.2  0.4  1.   0.2  0.4 28.2  0.4]\n",
      " [ 0.   0.   0.4  1.2  0.6  0.4  0.   0.6  1.6 31.2]]\n"
     ]
    }
   ],
   "source": [
    "# Train DecisionForest(n_trees=10, classes=np.unique(digits.target))\n",
    "# for all 10 digits simultaneously.\n",
    "# Compute and plot the confusion matrix after 5-fold cross-validation and comment on your results.\n",
    "# your code here\n",
    "features = digits.data\n",
    "labels = digits.target\n",
    "def classification_forest_arbitrary(features,labels,n_folds=5,n_trees=10):\n",
    "    accuracy_scores = [ ]\n",
    "    confusion_matrices = []\n",
    "    kfold = KFold(n_folds)\n",
    "    for train_index, test_index in kfold.split(features):\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "        forest = ClassificationForest(n_trees,np.unique(digits.target))\n",
    "        forest.train(X_train, y_train)\n",
    "\n",
    "        y_pred = [forest.predict(x) for x in X_test]\n",
    "\n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        confusion_matrices.append(cm)\n",
    "        #accuracy_scores+=accuracy\n",
    "\n",
    "    average_accuracy = np.mean(accuracy_scores)\n",
    "    average_cm = np.mean(confusion_matrices, axis=0)\n",
    "    return average_accuracy,average_cm\n",
    "\n",
    "accuracy_classficationforest,matrix=classification_forest_arbitrary(features,labels)\n",
    "print('Classification Forest(arbitrary):',accuracy_classficationforest)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(matrix)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "marks = np.arange(len(np.unique(digits.target)))\n",
    "plt.xticks(marks, np.unique(digits.target))\n",
    "plt.yticks(marks, np.unique(digits.target))\n",
    "plt.show()\n",
    "\n",
    "# Comment on the results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-against-the-rest classification with RegressionForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ten one-against-the-rest regression forests for the 10 digits.\n",
    "# Make sure that all training sets are balanced between the current digit and the rest.\n",
    "# Assign test instances to the digit with highest score, \n",
    "# or to \"unknown\" if all scores are negative.\n",
    "# Compute and plot the confusion matrix after 5-fold cross-validation and comment on your results.\n",
    "# your code here\n",
    "features = digits.data\n",
    "labels = digits.target\n",
    "\n",
    "def one_against_rest_regression_forest(features, labels, n_folds=5, n_trees=10):\n",
    "    accuracy_scores = []\n",
    "    confusion_matrices = []\n",
    "\n",
    "    for target_class in np.unique(labels):\n",
    "        binary_labels = np.where(labels == target_class, 1, -1)\n",
    "\n",
    "        kfold = KFold(n_folds)\n",
    "        for train_index, test_index in kfold.split(features):\n",
    "            X_train, X_test = features[train_index], features[test_index]\n",
    "            y_train, y_test = binary_labels[train_index], binary_labels[test_index]\n",
    "\n",
    "            forest = RegressionForest(n_trees)\n",
    "            forest.train(X_train, y_train)\n",
    "\n",
    "            y_pred = [forest.predict(x) for x in X_test]\n",
    "\n",
    "            if all(score < 0 for score in y_pred):\n",
    "                ypred_labels = \"unknown\"\n",
    "            else:\n",
    "                best_class = np.argmax(y_pred)\n",
    "                ypred_labels = np.unique(labels)[best_class]\n",
    "\n",
    "            accuracy = np.mean(ypred_labels == labels[test_index])\n",
    "            accuracy_scores.append(accuracy)\n",
    "\n",
    "            cm = confusion_matrix(labels[test_index], ypred_labels, labels=np.unique(labels))\n",
    "            confusion_matrices.append(cm)\n",
    "\n",
    "    average_accuracy = np.mean(accuracy_scores)\n",
    "    average_cm = np.mean(confusion_matrices, axis=0)\n",
    "    return average_accuracy, average_cm\n",
    "\n",
    "accuracy, matrix = one_against_rest_regression_forest(features, labels)\n",
    "print('Regression Forest :', accuracy)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(matrix)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "marks = np.arange(len(np.unique(labels)))\n",
    "plt.xticks(marks, np.unique(labels))\n",
    "plt.yticks(marks, np.unique(labels))\n",
    "plt.show()\n",
    "\n",
    "# Comment on the results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(matrix)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
