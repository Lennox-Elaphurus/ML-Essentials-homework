{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Authors:\n",
    "- Tuoxing Liu\n",
    "- Sima Esmaeili\n",
    "- Shruti Ghargi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate: 0.148\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "class ReLULayer(object):\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        relu = np.maximum(0, input)\n",
    "        return relu\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # Applying G_l(downstream) = G_l(upstream)·∂Zl/∂Zl−1\n",
    "        # Derivative of ReLU is 1 for input>0, hence self.input > 0\n",
    "        downstream_gradient = (self.input > 0) * upstream_gradient\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass \n",
    "\n",
    "class OutputLayer(object):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        exps = np.exp(input - np.max(input, axis=-1, keepdims=True))\n",
    "        softmax = exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "        return softmax\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_labels):\n",
    "        n = predicted_posteriors.shape[0]\n",
    "        # Derivative of cross-entropy loss with softmax\n",
    "        predicted_posteriors[range(n), true_labels] -= 1 # need to do copy before this\n",
    "        # Applying G_l(upstream)=∂Loss/∂Zl\n",
    "        downstream_gradient = predicted_posteriors / n\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass \n",
    "\n",
    "class LinearLayer(object):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.n_inputs  = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.B = np.random.normal(size=(n_inputs, n_outputs))  \n",
    "        self.b = np.random.normal(size=n_outputs) \n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        preactivations = np.dot(input, self.B) + self.b\n",
    "        return preactivations\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # ∂Loss/∂b = G_l(upstream) as ∂Zl/∂b = 1\n",
    "        self.grad_b = np.sum(upstream_gradient, axis=0)\n",
    "        # ∂Loss/∂B = G_l(upstream)·∂Zl/∂B = X^T · G_l(upstream)\n",
    "        self.grad_B = np.dot(self.input.T, upstream_gradient)\n",
    "        # G_l(downstream) = G_l(upstream)·∂Zl/∂Zl−1 = G_l(upstream)·B^T\n",
    "        downstream_gradient = np.dot(upstream_gradient, self.B.T)\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        # update weights and biases based on gradients calculated in backward step\n",
    "        self.B -= learning_rate * self.grad_B\n",
    "        self.b -= learning_rate * self.grad_b\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, n_features, layer_sizes):\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        self.layers   = []\n",
    "        n_in = n_features\n",
    "        for n_out in layer_sizes[:-1]:\n",
    "            self.layers.append(LinearLayer(n_in, n_out))\n",
    "            self.layers.append(ReLULayer())\n",
    "            n_in = n_out\n",
    "        n_out = layer_sizes[-1]\n",
    "        self.layers.append(LinearLayer(n_in, n_out))\n",
    "        self.layers.append(OutputLayer(n_out))\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size = X.shape[0]\n",
    "        # flatten the other dimensions of X \n",
    "        X = X.reshape(batch_size, -1)\n",
    "        result = X\n",
    "        for layer in self.layers:\n",
    "            result = layer.forward(result)\n",
    "        return result\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_classes):\n",
    "        downstream_gradient = self.layers[-1].backward(predicted_posteriors, true_classes)  # call OutputLayer backward first with true_labels\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            downstream_gradient = layer.backward(downstream_gradient)\n",
    "\n",
    "    def update(self, X, Y, learning_rate):\n",
    "        posteriors = self.forward(X)\n",
    "        self.backward(posteriors, Y)\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "\n",
    "    def train(self, x, y, n_epochs, batch_size, learning_rate):\n",
    "        N = len(x)\n",
    "        n_batches = N // batch_size\n",
    "        for i in range(n_epochs):\n",
    "            permutation = np.random.permutation(N)\n",
    "            for batch in range(n_batches):\n",
    "                start = batch * batch_size\n",
    "                x_batch = x[permutation[start:start+batch_size]]\n",
    "                y_batch = y[permutation[start:start+batch_size]]\n",
    "                self.update(x_batch, y_batch, learning_rate)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    N = 2000\n",
    "    X_train, Y_train = datasets.make_moons(N, noise=0.05)\n",
    "    X_test,  Y_test  = datasets.make_moons(N, noise=0.05)\n",
    "    n_features = 2\n",
    "    n_classes  = 2\n",
    "    offset  = X_train.min(axis=0)\n",
    "    scaling = X_train.max(axis=0) - offset\n",
    "    X_train = ((X_train - offset) / scaling - 0.5) * 2.0\n",
    "    X_test  = ((X_test  - offset) / scaling - 0.5) * 2.0\n",
    "    layer_sizes = [5, 5, n_classes]\n",
    "    n_epochs = 5\n",
    "    batch_size = 200\n",
    "    learning_rate = 0.05\n",
    "    network = MLP(n_features, layer_sizes)\n",
    "    network.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
    "    predicted_posteriors = network.forward(X_test)\n",
    "    predicted_classes = np.argmax(predicted_posteriors, axis=1)\n",
    "    error_rate = np.mean(predicted_classes != Y_test)\n",
    "    print(\"error rate:\", error_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Essentials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
