{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edc5f96a-f4d3-4cf8-b16e-fad25657ada3",
   "metadata": {},
   "source": [
    "## ex3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07046117-eab1-4994-8963-71e8ad3dd203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train Loss: 0.2919 | Train Accuracy: 91.43%\n",
      "Test Loss: 0.1607 | Test Accuracy: 95.12%\n",
      "\n",
      "Epoch 2/10:\n",
      "Train Loss: 0.1333 | Train Accuracy: 96.06%\n",
      "Test Loss: 0.1052 | Test Accuracy: 96.74%\n",
      "\n",
      "Epoch 3/10:\n",
      "Train Loss: 0.0889 | Train Accuracy: 97.35%\n",
      "Test Loss: 0.0808 | Test Accuracy: 97.48%\n",
      "\n",
      "Epoch 4/10:\n",
      "Train Loss: 0.0654 | Train Accuracy: 98.03%\n",
      "Test Loss: 0.0784 | Test Accuracy: 97.53%\n",
      "\n",
      "Epoch 5/10:\n",
      "Train Loss: 0.0510 | Train Accuracy: 98.48%\n",
      "Test Loss: 0.0699 | Test Accuracy: 97.81%\n",
      "\n",
      "Epoch 6/10:\n",
      "Train Loss: 0.0391 | Train Accuracy: 98.82%\n",
      "Test Loss: 0.0676 | Test Accuracy: 97.87%\n",
      "\n",
      "Epoch 7/10:\n",
      "Train Loss: 0.0302 | Train Accuracy: 99.12%\n",
      "Test Loss: 0.0663 | Test Accuracy: 97.89%\n",
      "\n",
      "Epoch 8/10:\n",
      "Train Loss: 0.0235 | Train Accuracy: 99.40%\n",
      "Test Loss: 0.0591 | Test Accuracy: 98.09%\n",
      "\n",
      "Epoch 9/10:\n",
      "Train Loss: 0.0188 | Train Accuracy: 99.52%\n",
      "Test Loss: 0.0612 | Test Accuracy: 98.10%\n",
      "\n",
      "Epoch 10/10:\n",
      "Train Loss: 0.0151 | Train Accuracy: 99.64%\n",
      "Test Loss: 0.0597 | Test Accuracy: 98.09%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class PRelu(nn.Module):\n",
    "    def __init__(self, num_parameters=1):\n",
    "        super(PRelu, self).__init__()\n",
    "        self.num_parameters = num_parameters\n",
    "        self.a = nn.Parameter(torch.Tensor(num_parameters))\n",
    "        self.a.data.fill_(0.25)  # Initialize with a default value of 0.25\n",
    "\n",
    "    def forward(self, x):\n",
    "        pos = torch.nn.functional.relu(x)\n",
    "        neg = self.a * (x - abs(x)) * 0.5\n",
    "        return pos + neg\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.prelu1 = PRelu(256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.prelu1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    accuracy = 100.0 * correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100.0 * correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def main():\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Initialize the model and optimizer\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.2f}%\")\n",
    "        print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
    "        print()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f8de86e-d7bc-416b-8c61-fdd259eb58aa",
   "metadata": {},
   "source": [
    "comparison with pervious module: \n",
    "\n",
    "ex3 introduces the Parametric ReLU activation function, which enhances the traditional ReLU by incorporating a learnable weight vector. This allows for adaptive activation patterns and introduces additional flexibility to the model. On the other hand, ex2 implements the dropout technique, which randomly sets elements of the input to zero during training. This helps prevent overfitting by encouraging the model to learn more robust representations and reduces reliance on specific features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
