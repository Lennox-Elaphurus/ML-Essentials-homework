{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d, cross_entropy\n",
    "\n",
    "plt.rc(\"figure\", dpi=100)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "# transform images into normalized tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "\n",
    "def init_weights(shape):\n",
    "    # Kaiming He initialization (a good initialization is important)\n",
    "    # https://arxiv.org/abs/1502.01852\n",
    "    std = np.sqrt(2. / shape[0])\n",
    "    w = torch.randn(size=shape) * std\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "\n",
    "def rectify(x):\n",
    "    # Rectified Linear Unit (ReLU)\n",
    "    return torch.max(torch.zeros_like(x), x)\n",
    "\n",
    "class RMSprop(optim.Optimizer):\n",
    "    \"\"\"\n",
    "    This is a reduced version of the PyTorch internal RMSprop optimizer\n",
    "    It serves here as an example\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # state initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(grad, avg, value=-group['lr'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the neural network\n",
    "def model(x, w_h, w_h2, w_o):\n",
    "    h = rectify(x @ w_h)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can't optimize a non-leaf Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m w_o \u001b[39m=\u001b[39m init_weights((\u001b[39m625\u001b[39m, \u001b[39m10\u001b[39m))\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m \u001b[39m# output shape is (B, 10)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m optimizer \u001b[39m=\u001b[39m RMSprop(params\u001b[39m=\u001b[39m[w_h, w_h2, w_o])\n\u001b[0;32m     14\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m     16\u001b[0m train_loss \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[1], line 79\u001b[0m, in \u001b[0;36mRMSprop.__init__\u001b[1;34m(self, params, lr, alpha, eps)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, params, lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, alpha\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, eps\u001b[39m=\u001b[39m\u001b[39m1e-8\u001b[39m):\n\u001b[0;32m     78\u001b[0m     defaults \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(lr\u001b[39m=\u001b[39mlr, alpha\u001b[39m=\u001b[39malpha, eps\u001b[39m=\u001b[39meps)\n\u001b[1;32m---> 79\u001b[0m     \u001b[39msuper\u001b[39m(RMSprop, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(params, defaults)\n",
      "File \u001b[1;32md:\\Miniconda\\ML_Essentials\\Lib\\site-packages\\torch\\optim\\optimizer.py:192\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    189\u001b[0m     param_groups \u001b[39m=\u001b[39m [{\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: param_groups}]\n\u001b[0;32m    191\u001b[0m \u001b[39mfor\u001b[39;00m param_group \u001b[39min\u001b[39;00m param_groups:\n\u001b[1;32m--> 192\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_param_group(param_group)\n\u001b[0;32m    194\u001b[0m \u001b[39m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Miniconda\\ML_Essentials\\Lib\\site-packages\\torch\\optim\\optimizer.py:515\u001b[0m, in \u001b[0;36mOptimizer.add_param_group\u001b[1;34m(self, param_group)\u001b[0m\n\u001b[0;32m    512\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39moptimizer can only optimize Tensors, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mbut one of the params is \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mtypename(param))\n\u001b[0;32m    514\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (param\u001b[39m.\u001b[39mis_leaf \u001b[39mor\u001b[39;00m param\u001b[39m.\u001b[39mretains_grad):\n\u001b[1;32m--> 515\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt optimize a non-leaf Tensor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    517\u001b[0m \u001b[39mfor\u001b[39;00m name, default \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    518\u001b[0m     \u001b[39mif\u001b[39;00m default \u001b[39mis\u001b[39;00m required \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m param_group:\n",
      "\u001b[1;31mValueError\u001b[0m: can't optimize a non-leaf Tensor"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "\n",
    "# input shape is (B, 784)\n",
    "w_h = init_weights((784, 625)).to(device)\n",
    "# hidden layer with 625 neurons\n",
    "w_h2 = init_weights((625, 625)).to(device)\n",
    "# hidden layer with 625 neurons\n",
    "w_o = init_weights((625, 10)).to(device)\n",
    "# output shape is (B, 10)\n",
    "\n",
    "optimizer = RMSprop(params=[w_h, w_h2, w_o])\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for epoch in range(n_epochs + 1):\n",
    "    train_loss_this_epoch = []\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        x, y = batch\n",
    "\n",
    "        # our model requires flattened input\n",
    "        x = x.reshape(batch_size, 784).to(device)\n",
    "        # feed input through model\n",
    "        noise_py_x = model(x, w_h, w_h2, w_o)\n",
    "\n",
    "        # reset the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y=y.to(device)\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        loss = cross_entropy(noise_py_x, y, reduction=\"mean\").to(\"cpu\")\n",
    "\n",
    "        train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "    # test periodically\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "        test_loss_this_epoch = []\n",
    "\n",
    "        # no need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(test_dataloader):\n",
    "                x, y = batch\n",
    "                x = x.reshape(batch_size, 784).to(device)\n",
    "                noise_py_x = model(x, w_h, w_h2, w_o)\n",
    "\n",
    "                y=y.to(device)\n",
    "                loss = cross_entropy(noise_py_x, y, reduction=\"mean\").to(\"cpu\")\n",
    "                test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "        print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "\n",
    "plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "plt.title(\"Train and Test Loss over Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Essentials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
